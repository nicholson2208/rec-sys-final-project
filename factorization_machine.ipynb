{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478edc77-b3fd-4bdf-bd00-ba6767fce6d9",
   "metadata": {},
   "source": [
    "# Running factorization machine\n",
    "\n",
    "### We're likely running an FM instead of a matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1f78624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('/FactorizationMachine_modules')\n",
    "\n",
    "from FactorizationMachine_modules import eval_utils_fact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65285f-c18d-4eda-964b-ec702a74529b",
   "metadata": {},
   "source": [
    "### These are older files and will revisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2de63efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_test/test.pkl\", \"rb\") as pfile:\n",
    "    test = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "61382e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_test/train.pkl\", \"rb\") as pfile:\n",
    "    train = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "36388a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_test/anti_test.pkl\", \"rb\") as pfile:\n",
    "    anti_test = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59fab9",
   "metadata": {},
   "source": [
    "### Factorization machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6731b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/coreylynch/pyFM\n",
      "  Cloning https://github.com/coreylynch/pyFM to /tmp/pip-req-build-sa4cxitt\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/coreylynch/pyFM /tmp/pip-req-build-sa4cxitt\n",
      "  Resolved https://github.com/coreylynch/pyFM to commit 0696c980993889a9429e4ab0b6c7dc8be6dac4de\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/coreylynch/pyFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d56b5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from pyfm import pylibfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdfc9b",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2bc7424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename,path=\"\", sample=1.0):\n",
    "    data = []\n",
    "    y = []\n",
    "    users=set()\n",
    "    items=set()\n",
    "    with open(path+filename) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            (index,user,item,rating)=line.split(',')\n",
    "            if random.random() <= sample:\n",
    "                data.append({ \"user\": str(user), \"item\": str(item)})\n",
    "                y.append(float(rating))\n",
    "                users.add(user)\n",
    "                items.add(item)\n",
    "\n",
    "    return (data, np.array(y), users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3352a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, y_train, train_users, train_items) = loadData(\"train_test/train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "274bc04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_data, y_test, test_users, test_items) = loadData(\"train_test/test_df.csv\") #contains both test and anti-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f4dc39af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': 'cqz', 'item': 'jbigham'},\n",
       " {'user': 'cqz', 'item': 'ryanatkn'},\n",
       " {'user': 'cqz', 'item': 'axz'},\n",
       " {'user': 'cqz', 'item': 'msbernst'},\n",
       " {'user': 'cqz', 'item': 'qli'}]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f03630aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': 'cqz', 'item': 'kentrellowens'},\n",
       " {'user': 'cqz', 'item': 'ruotongw'},\n",
       " {'user': 'cqz', 'item': 'schaferj'},\n",
       " {'user': 'Gillian', 'item': 'kgajos'},\n",
       " {'user': 'Gillian', 'item': 'andreaforte'}]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a5788-f809-4df8-844a-4a0337d6138f",
   "metadata": {},
   "source": [
    "### This is needed to collect the data in fm format\n",
    "\n",
    "This is constructing a full matrix where our rows are our users and the columns is every single pair of their connections. \n",
    "List of dictionaries that are user (person) and the items (every person that they could follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4741bab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Self-edges? Is this something we are addressing. We shouldn't have MattNicholson --> MattNicholson\n",
    "from itertools import permutations\n",
    "\n",
    "#convert the combination_result to sets\n",
    "X_train_data_extended = [{'user': pair[0], 'item': pair[1]} for pair in permutations(list(train_users), 2)]\n",
    "# X_train_data_extended_tuples = [(u, v) for u, v in permutations(train_users, 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1886264d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759512"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_data_extended) #this contains all the possible edge options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c1805103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if value in train_data\n",
    "{'user': 'cqz', 'item': 'jbigham'} in X_train_data_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da89885-62f7-45c3-a8b9-023af8c3c91e",
   "metadata": {},
   "source": [
    "### Produce our training examples \n",
    "These are all 1's or 0's and it is the length of all permutations in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "973494e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_extended_list = []\n",
    "\n",
    "for comb in X_train_data_extended:\n",
    "    if comb in train_data:\n",
    "        y_train_extended_list.append(1)\n",
    "    else:\n",
    "        y_train_extended_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "992e0398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759512"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_extended_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "020a6ddf-6615-4d3b-8a7f-9253bf394d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0 737032]\n",
      " [     1  22480]]\n"
     ]
    }
   ],
   "source": [
    "# We are populating our `y_train_extended_list` with both 1s and 0s\n",
    "\n",
    "unique, counts = np.unique(y_train_extended_list, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b98cbf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22480\n",
      "22480\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(y_train_extended_list.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bee3af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759512\n",
      "759512\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_data_extended))\n",
    "print(len(y_train_extended_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "26d4925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to an array for pylib.FM\n",
    "y_train_data_extended = np.array(y_train_extended_list, dtype='double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "eb3cd7e2-3dfc-4367-9a6a-c6d20f90fa73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('train_test/y_train_extended_list.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_extended_list, f)\n",
    "    \n",
    "with open('train_test/X_train_data_extended.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_data_extended, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59008d8",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6de9a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(X_train_data_extended)\n",
    "X_test = v.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "01c45c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.01335\n",
      "-- Epoch 2\n",
      "Training MSE: 0.01264\n",
      "-- Epoch 3\n",
      "Training MSE: 0.01252\n",
      "-- Epoch 4\n",
      "Training MSE: 0.01247\n",
      "-- Epoch 5\n",
      "Training MSE: 0.01243\n"
     ]
    }
   ],
   "source": [
    "fm = pylibfm.FM (num_factors=10, \n",
    "                 num_iter=5, \n",
    "                 verbose=True, \n",
    "                 task=\"regression\", \n",
    "                 initial_learning_rate=0.001, \n",
    "                 learning_rate_schedule=\"optimal\")\n",
    "\n",
    "fm.fit(X_train, y_train_data_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b39d08ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37498284809685056"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = fm.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3f428",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "762da426-a841-448e-9276-5617ca3727b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in /opt/conda/lib/python3.10/site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in /opt/conda/lib/python3.10/site-packages (from surprise) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5d7202c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from surprise import SVD, Reader, Dataset, Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8103f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_len = 5 # number of recommendations to return\n",
    "predict_list_len = 100\n",
    "frac = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2ed97e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recs = eval_utils_fact.create_test_recommendations(fm.predict, v, test_data, list_len, train_items, predict_list_len, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a3b10074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shamsi',\n",
       "  [('shamsi', 'cfiesler', 0.4727124816512766),\n",
       "   ('shamsi', 'sigchi', 0.4303673166166387),\n",
       "   ('shamsi', 'bkeegan', 0.39614376614411334),\n",
       "   ('shamsi', 'Heycori', 0.2297964418145225),\n",
       "   ('shamsi', 'asb', 0.22318965331158053)]),\n",
       " ('karthik',\n",
       "  [('karthik', 'sigchi', 0.3375504959488091),\n",
       "   ('karthik', 'drmaxlwilson', 0.2211408876784548),\n",
       "   ('karthik', 'barik', 0.1881697207880393),\n",
       "   ('karthik', 'Heycori', 0.13247840525846277),\n",
       "   ('karthik', 'asb', 0.11952407530533524)]),\n",
       " ('panciera',\n",
       "  [('panciera', 'jbigham', 0.5338622942015655),\n",
       "   ('panciera', 'cfiesler', 0.4419223970472083),\n",
       "   ('panciera', 'andresmh', 0.4314979784328827),\n",
       "   ('panciera', 'msbernst', 0.3453838369952485),\n",
       "   ('panciera', 'sigchi', 0.34097296224737034)])]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the test_recs look like?\n",
    "list(test_recs.iter_recs())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e89f596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg = NDCGEvaluator(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "10ae3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg.setup(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "412cef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg.evaluate(test_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "10af3eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11497133369753697"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "285dab03-f03b-42de-8af0-d3a80bf61cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = PrecisionEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "efb9375b-7a00-48de-9611-6ddb31226cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.setup(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "772d08af-6bcd-4e60-8866-78d5de5e94a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shamsi',\n",
       "  [('shamsi', 'cfiesler', 0.4727124816512766),\n",
       "   ('shamsi', 'sigchi', 0.4303673166166387),\n",
       "   ('shamsi', 'bkeegan', 0.39614376614411334),\n",
       "   ('shamsi', 'Heycori', 0.2297964418145225),\n",
       "   ('shamsi', 'asb', 0.22318965331158053)]),\n",
       " ('karthik',\n",
       "  [('karthik', 'sigchi', 0.3375504959488091),\n",
       "   ('karthik', 'drmaxlwilson', 0.2211408876784548),\n",
       "   ('karthik', 'barik', 0.1881697207880393),\n",
       "   ('karthik', 'Heycori', 0.13247840525846277),\n",
       "   ('karthik', 'asb', 0.11952407530533524)]),\n",
       " ('panciera',\n",
       "  [('panciera', 'jbigham', 0.5338622942015655),\n",
       "   ('panciera', 'cfiesler', 0.4419223970472083),\n",
       "   ('panciera', 'andresmh', 0.4314979784328827),\n",
       "   ('panciera', 'msbernst', 0.3453838369952485),\n",
       "   ('panciera', 'sigchi', 0.34097296224737034)])]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_recs.iter_recs())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e8128ecf-53f6-4609-8240-bdf853fad3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision.evaluate(test_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bf4c6322-b7a0-4f48-bee6-ef0065e9259b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48505747126436793"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c2b9a-8e6c-45a3-9297-6b0a9bda6865",
   "metadata": {},
   "source": [
    "### Cross-validatoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8e89b429-be73-4989-a97a-43cede4a845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold-0\n",
      "done reading the data\n",
      "done extending the data\n",
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.02204\n",
      "-- Epoch 2\n",
      "Training MSE: 0.02080\n",
      "-- Epoch 3\n",
      "Training MSE: 0.02058\n",
      "-- Epoch 4\n",
      "Training MSE: 0.02048\n",
      "-- Epoch 5\n",
      "Training MSE: 0.02043\n",
      "done making matrix factorization recs\n",
      "NDCG: 0.13955004586180644 \t Precision: 0.7534013605442177\n",
      "Starting fold-1\n",
      "done reading the data\n",
      "done extending the data\n",
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.02204\n",
      "-- Epoch 2\n",
      "Training MSE: 0.02080\n",
      "-- Epoch 3\n",
      "Training MSE: 0.02057\n",
      "-- Epoch 4\n",
      "Training MSE: 0.02048\n",
      "-- Epoch 5\n",
      "Training MSE: 0.02043\n",
      "done making matrix factorization recs\n",
      "NDCG: 0.14015018989515615 \t Precision: 0.7387755102040816\n",
      "Starting fold-2\n",
      "done reading the data\n",
      "done extending the data\n",
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.02200\n",
      "-- Epoch 2\n",
      "Training MSE: 0.02072\n",
      "-- Epoch 3\n",
      "Training MSE: 0.02049\n",
      "-- Epoch 4\n",
      "Training MSE: 0.02040\n",
      "-- Epoch 5\n",
      "Training MSE: 0.02035\n",
      "done making matrix factorization recs\n",
      "NDCG: 0.1399990318175022 \t Precision: 0.7418367346938775\n",
      "Starting fold-3\n",
      "done reading the data\n",
      "done extending the data\n",
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.02202\n",
      "-- Epoch 2\n",
      "Training MSE: 0.02077\n",
      "-- Epoch 3\n",
      "Training MSE: 0.02054\n",
      "-- Epoch 4\n",
      "Training MSE: 0.02045\n",
      "-- Epoch 5\n",
      "Training MSE: 0.02040\n",
      "done making matrix factorization recs\n",
      "NDCG: 0.14004698291633721 \t Precision: 0.7608843537414965\n",
      "Starting fold-4\n",
      "done reading the data\n",
      "done extending the data\n",
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.02207\n",
      "-- Epoch 2\n",
      "Training MSE: 0.02084\n",
      "-- Epoch 3\n",
      "Training MSE: 0.02061\n",
      "-- Epoch 4\n",
      "Training MSE: 0.02052\n",
      "-- Epoch 5\n",
      "Training MSE: 0.02047\n",
      "done making matrix factorization recs\n",
      "NDCG: 0.14271369316591886 \t Precision: 0.7401360544217687\n"
     ]
    }
   ],
   "source": [
    "hci_ndcg_scores = []\n",
    "hci_precision_scores = []\n",
    "k = 5\n",
    "\n",
    "list_len = 5 # number of recommendations to return\n",
    "predict_list_len = 5\n",
    "frac = 1\n",
    "\n",
    "fm = pylibfm.FM (num_factors=10, \n",
    "                 num_iter=5, \n",
    "                 verbose=True, \n",
    "                 task=\"regression\", \n",
    "                 initial_learning_rate=0.001, \n",
    "                 learning_rate_schedule=\"optimal\")\n",
    "\n",
    "for fold in [\"-0\", \"-1\", \"-2\", \"-3\", \"-4\"]:\n",
    "    \n",
    "    print(\"Starting fold{}\".format(fold))\n",
    "    \n",
    "    with open(\"train_test/new_folds/train{}.pkl\".format(fold), \"rb\") as pfile: \n",
    "        this_fold_train = pickle.load(pfile)\n",
    "    \n",
    "    with open(\"train_test/new_folds/test{}.pkl\".format(fold), \"rb\") as pfile: \n",
    "        this_fold_test = pickle.load(pfile)\n",
    "        \n",
    "    with open(\"train_test/new_folds/anti_test{}.pkl\".format(fold), \"rb\") as pfile: \n",
    "        this_fold_anti_test = pickle.load(pfile)\n",
    "    \n",
    "    print(\"done reading the data\")\n",
    "    \n",
    "    \n",
    "    #prepare the data\n",
    "    train_data, test_data = eval_utils_fact.create_train_test_df(this_fold_train, this_fold_test, this_fold_anti_test)\n",
    "\n",
    "    #load data\n",
    "    (train_data, y_train, train_users, train_items) = eval_utils_fact.loadData(train_data)\n",
    "    (test_data, y_test, test_users, test_items) = eval_utils_fact.loadData(test_data)\n",
    "    \n",
    "    #extending data\n",
    "    X_train_data_extended, y_train_data_extended = eval_utils_fact.extend_data(train_data, test_data, train_users)\n",
    "    \n",
    "    print(\"done extending the data\")\n",
    "    \n",
    "    #vectorize\n",
    "    v = DictVectorizer()\n",
    "    X_train = v.fit_transform(X_train_data_extended)\n",
    "    X_test = v.transform(test_data)\n",
    "    \n",
    "    #train the model\n",
    "    fm.fit(X_train, y_train_data_extended)\n",
    "    \n",
    "    preds = fm.predict(X_test)\n",
    "    test_recs = eval_utils_fact.create_test_recommendations(fm.predict, v, test_data, list_len, train_items, predict_list_len, frac)\n",
    "    \n",
    "    print(\"done making matrix factorization recs\")\n",
    "    \n",
    "    # --- evaluate ----\n",
    "    \n",
    "    # NDCG\n",
    "    this_fold_ndcg_hci = NDCGEvaluator(k=k)\n",
    "    this_fold_ndcg_hci.setup(trainset=train_data, testset=test_data)\n",
    "    this_fold_ndcg_hci.evaluate(test_recs)\n",
    "    hci_ndcg_scores.append(this_fold_ndcg_hci.score)\n",
    "    \n",
    "    # Precision\n",
    "    this_fold_precision_hci = PrecisionEvaluator()\n",
    "    this_fold_precision_hci.setup(trainset=train_data, testset=test_data)\n",
    "    this_fold_precision_hci.evaluate(test_recs)\n",
    "    hci_precision_scores.append(this_fold_precision_hci.score)\n",
    "    \n",
    "    \n",
    "    print(\"NDCG: {} \\t Precision: {}\".format(this_fold_ndcg_hci.score, this_fold_precision_hci.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e644b9bc-b173-4ad9-8e06-543c4d12bb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13955004586180644,\n",
       " 0.14015018989515615,\n",
       " 0.1399990318175022,\n",
       " 0.14004698291633721,\n",
       " 0.14271369316591886]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hci_ndcg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "907d356c-5850-4cdc-a7e7-e5173245899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7534013605442177,\n",
       " 0.7387755102040816,\n",
       " 0.7418367346938775,\n",
       " 0.7608843537414965,\n",
       " 0.7401360544217687]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hci_precision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35c670-da2c-46e9-8377-d4b502ce3d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
